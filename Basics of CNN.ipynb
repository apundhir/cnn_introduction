{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca9e859c",
   "metadata": {},
   "source": [
    "## Convolution Neural Network (CNN)\n",
    "\n",
    "*Convolutional Neural Networks, or CNNs, are specialised architectures which work particularly well with visual data, i.e. images and videos (though not restricted to them). They have been largely responsible for revolutionalizing 'deep learning' by setting new benchmarks for many image processing tasks that were very recently considered extremely hard. They are very similar to the vanilla neural networks (multilayer perceptrons) - every neuron in one layer is connected to every neuron in the next layer, they follow the same general principles of forward and backpropagation, etc. However, there are certain features of CNNs that make them perform extremely well on image processing tasks.*\n",
    "\n",
    "Although the vanilla neural networks (MLPs) can learn extremely complex functions, their architecture does not exploit what we know about how the brain reads and processes images. For this reason, although MLPs are successful in solving many complex problems, they haven't been able to achieve any major breakthroughs in the image processing domain.\n",
    "\n",
    "Some of the common challenges in image processing are:\n",
    "\n",
    "1. Viewpoint variation: Different orientations of the image with respect to the camera.\n",
    "\n",
    "\n",
    "![image.png](./resources/viewpoint+variation.png)\n",
    "\n",
    "\n",
    "2. Scale variation: Different sizes of the object with respect to the image size.\n",
    "\n",
    "![image.png](./resources/scale+variation.png)\n",
    "    \n",
    "\n",
    "3. Illumination conditions: Illumination effects.\n",
    "\n",
    "\n",
    "![image.png](./resources/illumination.png)\n",
    "    \n",
    "\n",
    "4. Background clutter: Varying backgrounds.\n",
    "\n",
    "\n",
    "![image.png](./resources/background.png)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee0c75",
   "metadata": {},
   "source": [
    "**The ImageNet Challenge**\n",
    "\n",
    "CNNs had first demonstrated their extraordinary performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).  The ILSVRC uses a list of about 1000 image categories or \"classes\" and has about 1.2 million training images. The original challenge is an image classification task. \n",
    "\n",
    "![image.png](./resources/imagenet.png)\n",
    "\n",
    "\n",
    "You can see the impressive results of CNNs in the ILSVRC where they now outperform humans (having 5% error rate). The error rate of the ResNet, a recent variant in the CNN family, is close to 3%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cccda7",
   "metadata": {},
   "source": [
    "Applications of CNN:\n",
    "\n",
    "1. Object Localization\n",
    "\n",
    "![image.png](./resources/yolo.png)\n",
    "\n",
    "2. Semantic Segmentation\n",
    "\n",
    "![image.png](./resources/image_segmentation.png)\n",
    "\n",
    "3. Optical Character Recognition (OCR)\n",
    "\n",
    "![image.png](./resources/ocr.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed42273",
   "metadata": {},
   "source": [
    "Evolution of CNN: Architecture of CNNs is motivated by the visual system of mammals. In this segment, we will discuss an influential paper named “Receptive field for single neurons in the cat’s striate cortex” published by Hubel and Wiesel.\n",
    "\n",
    "This was basically a bunch of experiments conducted to understand the visual system of a cat. In the experiments, spots of light (of various shapes and size) were made to fall on the retina of a cat and, using an appropriate mechanism, the response of the neurons in the cat's retina was recorded. This provided a way to observe which types of spots make some particular neurons 'fire', how groups of neurons respond to spots of certain shapes, etc.\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf\n",
    "\n",
    "Some of the important observations made in the study were:\n",
    "\n",
    "1. Each neuron in the retina focuses on one part of the image and that part of the image is called the receptive field of that neuron.\n",
    "\n",
    "2. There are excitatory and inhibitory regions in the receptive field. The neurons only ‘fire’ when there is a contrast between the excitatory and the inhibitory regions. If we splash light over the excitatory and inhibitory regions together, because of no contrast between them, the neurons don’t ‘fire’ (respond). If we splash light just over the excitatory region, neurons respond because of the contrast.\n",
    "\n",
    "3. The strength of the response is proportional to the summation over only the excitatory region (not inhibitory region). Later, you will study the pooling layer in CNNs which corresponds to this observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6ee7a",
   "metadata": {},
   "source": [
    "**Using this idea, if we design a complex network with multiple layers to do image classification (for example), the layers in the network should do something like this:**\n",
    "\n",
    "1. The first layer extracts raw features, like vertical and horizontal edges\n",
    "\n",
    "2. The second layer extracts more abstract features such as textures (using the features extracted by the first layer)\n",
    "\n",
    "3. The subsequent layers may identify certain parts of the image such as skin, hair, nose, mouth etc. based on the textures.\n",
    "\n",
    "4. Layers further up may identify faces, limbs etc. \n",
    "\n",
    "5. Finally, the last layer may classify the image as 'human', 'cat' etc.\n",
    "\n",
    "![image.png](./resources/classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8873e7e",
   "metadata": {},
   "source": [
    "## VGGNet\n",
    "\n",
    "Let's dig a little deeper into CNN architectures now. In this segment, we will analyse the architecture of a popular CNN called VGGNet. This is one of the pure CNN architecture to understand basic concepts of CNN. Observing the VGGNet architecture will give you a high-level overview of the common types of CNN layers before you study each one of them in detail.\n",
    "\n",
    "![image.png](./resources/vgg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299d49f",
   "metadata": {},
   "source": [
    "There are three main type of layers in an CNN architecture:\n",
    "\n",
    "1. Convolution layers: These layers are responsible for shrinkage of input image size\n",
    "2. Pooling layers\n",
    "3. Feature maps\n",
    "\n",
    "\n",
    "The VGGNet was specially designed for the ImageNet challenge which is a classification task with 1000 categories. Thus, the softmax layer at the end has 1000 categories. The blue layers are the convolutional layers while the yellow ones are pooling layers. Finally, the green layer is a fully connected layer with 4096 neurons, the output from which is a vector of size 4096.\n",
    "\n",
    "![image.png](./resources/vggnet2.png)\n",
    "\n",
    "\n",
    "The most important point to notice is that the network acts as a feature extractor for images. For example, the CNN above extracts a 4096-dimensional feature vector representing each input image. In this case, the feature vector is fed to a softmax layer for classification, but you can use the feature vector to do other tasks as well (such as video analysis, object detection, image segmentation etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93889264",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Before going into convolution, lets first see a new term called Filter (also known as kernel). Filters are a kind of a matrix of numbers, which when put on image binary representation, shields out some of the numbers and enhance other part of it. As this filter is generally much smaller than image size, we need to move it over image from top left to bottom right.\n",
    "\n",
    "![image.png](./resources/convolve1.png)\n",
    "\n",
    "\n",
    "Mathematically, the convolution operation is the summation of the element-wise product of two matrices. Let’s take two matrices, X and Y. If you 'convolve the image X using the filter Y', this operation will produce the matrix Z. \n",
    "\n",
    "![image.png](./resources/convolve2.png)\n",
    "\n",
    "\n",
    "This process of moving filter over image and extracting features is known as convolution. Filters can be of different size or type (different filters to detect different properties of the image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8037f16",
   "metadata": {},
   "source": [
    "## Strides and Padding\n",
    "\n",
    "While doing convolution, we have to move filter over the image. We can move filter over image by an arbitrary number of steps. This number is called Stride.\n",
    "\n",
    "If you choose stride other than 1, you have to make sure filter fits on the image by adding extra dummy pixels around. This is known as padding.\n",
    "\n",
    "![image.png](./resources/stride.png)\n",
    "\n",
    "\n",
    "1. **when you convolve an image without padding (using any filter size), the output size is smaller than the image (i.e. the output 'shrinks'). For e.g. when you convolve a (6, 6) image with a (3, 3) filter and stride of 1, you get an output of (4, 4). If you want to maintain the same size, you can use padding.**\n",
    "2. **It is important to note that only the width and height decrease (not the depth) when you convolve without padding.  The depth of the output depends on the number of filters used**\n",
    "\n",
    "\n",
    "So far, we have been doing convolutions only on 2D arrays (images), say of size 6x6. But most real images are coloured (RGB) images and are 3D arrays of size m x n x 3. Generally, we represent an image as a 3D matrix of size height x width x channels.\n",
    "\n",
    "![image.png](./resources/3dfilter.png)\n",
    "\n",
    "To convolve such images, we simply use 3D filters. The basic idea of convolution is still the same - we take the element-wise product and sum up the values. The only difference is that now the filters will be 3-dimensional, for e.g. 3 x 3 x 3, or 5 x 5 x 3 (the last '3' represents the fact that the filter has as many channels as the image). \n",
    "\n",
    "\n",
    "1. We use 3D filters to perform convolution on 3D images. For e.g. if we have an image of size (224, 224, 3), we can use filters of sizes (3, 3, 3), (5, 5, 3), (7, 7, 3) etc. (with appropriate padding etc.). We can use a filter of any size as long as the number of channels in the filter is the same as that in the input image.\n",
    "2. **The filters are learnt during training** (i.e. during backpropagation). Hence, the individual values of the filters are often called the weights of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac5e45",
   "metadata": {},
   "source": [
    "## Feature Map\n",
    "\n",
    "1. A neuron is basically a filter whose weights are learnt during training. For example, a (3, 3, 3) filter (or neuron) has 27 weights. Each neuron looks at a particular region in the input (i.e. its 'receptive field').\n",
    "2. A feature map is a collection of multiple neurons each of which looks at different regions of the input with the same weights. All neurons in a feature map extract the same feature (but from different regions of the input). It is called a 'feature map' because it is a mapping of where a certain feature is found in the image. \n",
    "\n",
    "\n",
    "![image.png](./resources/featuremap1.png)\n",
    "\n",
    "\n",
    "Note: **feature map' refers to the (non-linear) output of the activation function, not what goes into the activation function (i.e. the output of the convolution).**\n",
    "\n",
    "\n",
    "Let's now revisit VGGNet architecture again, we can see that we are applying 64 filters on size (3,3,3) on an image of dimention(224,224,3) to create feature map tensor of size (224, 224, 64) containing 64 feature maps of size (224,224) as shown below.\n",
    "\n",
    "![image.png](./resources/vgg2.png)\n",
    "\n",
    "\n",
    "The (224, 224, 64) tensor is the output of the first convolutional layer. The 64 feature maps, or the (224, 224, 64) tensor, is then fed to a **pooling layer.**\n",
    "\n",
    "## Pooling\n",
    "\n",
    "CNNs typically aggregate these features using the pooling layer. Pooling tries to figure out whether a particular region in the image has the feature we are interested in or not. It essentially looks at larger regions (having multiple patches) of the image and captures an aggregate statistic (max, average etc.) of each region.\n",
    "\n",
    "The two most popular aggregate functions used in pooling are 'max' and 'average'. The intuition behind these are as follows:\n",
    "\n",
    "1. Max pooling: If any one of the patches says something strongly about the presence of a certain feature, then the pooling layer counts that feature as 'detected'.\n",
    "2. Average pooling: If one patch says something very firmly but the other ones disagree,  the pooling layer takes the average to find out.\n",
    "\n",
    "![image.png](./resources/pooling1.png)\n",
    "\n",
    "![image.png](./resources/pooling2.png)\n",
    "\n",
    "You can observe that pooling operates on each feature map independently. It reduces the size (width and height) of each feature map, but the number of feature maps remains constant. \n",
    "\n",
    " \n",
    "\n",
    "Pooling has the advantage of making the representation more compact by reducing the spatial size (height and width) of the feature maps, thereby reducing the number of parameters to be learnt. On the other hand, it also loses a lot of information, which is often considered a potential disadvantage. Having said that, pooling has empirically proven to improve the performance of most deep CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f1ce1",
   "metadata": {},
   "source": [
    "To summarise, a typical CNN has the following sequence of CNN layers:\n",
    "\n",
    "1. We have an input image which is convolved using multiple filters to create multiple feature maps\n",
    "2. Each feature map, of size (c, c), is pooled to generate a (c/2, c/2) output (for a standard 2 x 2 pooling). \n",
    "3. The above pattern is called a CNN layer or unit. Multiple such CNN layers are stacked on top of one another to create deep CNN networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718c303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
